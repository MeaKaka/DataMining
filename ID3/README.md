### ID3
分类决策树,本身就是一个多叉树<br>
ID3算法中根据特征选择和信息增益评估，每次选择信息增益最大的特征作为分支标准。ID3算法可用于划分标称型数据集，**没有剪枝** 过程，为了去除过度数据匹配的问题，可通过裁剪合并相邻的无法产生大量信息增益的叶子节点（例如设置信息增益阀值）。使用信息增益其实是有一个缺点，那就是它偏向于具有大量值的属性–就是说在训练集中，某个属性所取的不同值的个数越多，那么越有可能拿它来作为分裂属性，而这样做有时候是没有意义的。此外，ID3不能处理连续分布的数据特征<br>
匹配项不要过多<br>
信息熵增益作为分类标准。相同s的情况下，信息熵越小，表示样本对目标属性的分布越纯，效果越好。 (这样的结果从数学上分析就是可能取值最多的属性，概率越小每个值的log值的绝对值就越大)
```math
Entropy(s) = Entropy(p_1,p_2,...,p_n)=-p_1logp_1-p_2logp_2-...-p_nlogp_n

Gain(S,A) = Entropy(S) - Entropy_A(S)

Entropy_A(S) = P_{1A}Entropy(S_1)+ ... +P_{nA}Entropy(S_n)
```
问题：在决策树的高度不为1时，计算信息熵增益的过程能否以第一次的情况为分类依据<br>
答：这是当然可以的，因为在每次迭代过程中，选择的必定是当前可选择属性中每个分支概率取值最小的.
树成型中与概率有关，树成型后与概率无关。
###### 优化——C4.5
1. 将信息增益改为信息增益比<br>
```math
IGR = Gain/H 
```
H表示内在信息，内在信息可以看作是整体取值。比如说天气A和活动与否S，计算Entropy_A(S) 应该考虑不同天气对应活动与否的情况，而H则只需要考虑天气A这一属性内部取值的情况。引入这一变量，会出现当一个属性本身不确定性较大时，被选择的概率也就不高的情况。
2. 连续数据离散化<br>
3. 对缺失值进行了考虑，按比例进行权重调节，也可以求平均数吧？
4. 引入了正则化剪枝，C4.5算法采用PEP(Pessimistic Error Pruning)剪枝法。是一种自上而下的剪枝法，根据剪枝前后的错误率来判定是否进行子树的修剪。（将数据分为生长集和剪枝集）

###### 优化——CART   
决策树分为分类树和回归树两种<br>
分类树对离散变量做决策，输出是样本的预测类别；回归树对连续变量做决策，输出是一个实数<br>
return一棵二叉树，方便计算机运算<br>
gini指数
```math
Gini(A) = 1-\sum{{p_k}^2}
```
对于属性值较多的属性,对属性值进行排列组合,分类,然后**剪枝**
- 预剪枝<br>
    预剪枝是在决策树生成过程中，在划分节点时，若该节点的划分没有提高其在训练集上的准确率，则不进行划分。
- 后剪枝<br>
    - 将数据分为生长集和剪枝集
    - 常见方法有CCP(Cost Complexity Pruning)、REP(Reduced Error Pruning)、PEP(Pessimistic Error Pruning)、MEP(Minimum Error Pruning)
    - CART采用的是CCP(Cost Complexity Pruning)的剪枝法策略。
    - 后剪枝决策树比起预剪枝决策树保留了更多的分支。在一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但同时其训练时间花销也比较大。<br>
    
###### 编码
代码含有ID3和优化后的C4.5,不含剪枝过程
